## 1.引言
虽然我们知道，代价函数Jθ的表达式，但是还不知道怎么去确定假设函数hθ(x)的所有参数θ0,θ1 ... θn，使得Jθ值最小。

![](https://camo.githubusercontent.com/9b486198032d4371e83f37c39d0dbec6e12013dd/687474703a2f2f73747564656e7464656e672e6769746875622e696f2f696d616765732f6d6c2f31322e706e67)

我们还是假设hθ(x)是一个线性函数，并且只有2个参数θ0,θ1，对应特征向量x只有1维

我们的目标是使得J(θ0,θ1)最小

![](https://camo.githubusercontent.com/4c2a7fe0450db5a543dfebeb6194606e3bd78251/687474703a2f2f73747564656e7464656e672e6769746875622e696f2f696d616765732f6d6c2f312e706e67)

我们可以采用以下方法，尝试找到J(θ0,θ1)最小值

1. 给θ0, θ1一个初始值，例如都等于0
2. 不断改变θ0, θ1的值，并且满足J(θ0,θ1)递减，直到达到一个我们满意的最小值，此时θ0,θ1即我们所求的最佳参数值

这个算法我们称之为"梯度下降"算法

假设J(θ0,θ1)值和参数θ0,θ1满足如下3维关系图

![](http://studentdeng.github.io/images/ml/10.png)

按照前面提到的迭代方法，假设不同的θ0, θ1的初始值，我们可能会有如下两个不同的迭代过程

![](http://52opencourse.com/?qa=blob&qa_blobid=17796887071118187401)

![](http://studentdeng.github.io/images/ml/14.png)

从前面两个图中所示，我们会发现θ0, θ1初始值不同的时候，我们会找到不同局部最小值，这个特点正是"梯度下降"算法的特点，虽然很多时候都只会有一个全局的最小值

## 2.梯度下降-初步了解
根据前面的介绍，当只有2个参数θ0, θ1的时候，可以定义梯度下降算法的函数，如下

![](http://studentdeng.github.io/images/ml/5.png)

特别说明:

1. := 表示赋值，例如a := b 表示把b的值赋值给a
2. = 表示判断是否相等，例如 a = b表示判断a等于b
3. 重复上述过程，直到函数收敛，得到的θ0, θ1值即为最佳参数值
4. α我们称为learning rate，如果α的值太大则迭代的步伐太大，反应在图上即下降的速率太大，可能导致错过了局部最小值；如果α的值太小则迭代的步伐太小，反应在图上即下降的速率太小，会导致求解过程太慢。
5. 注意，我们发现等式右边被减数是一个 求偏导数，如果不了解偏导数（参考https://zh.wikipedia.org/wiki/%E5%81%8F%E5%AF%BC%E6%95%B0）
6. 另外还有一个需要注意的是，所有的参数必须需要同步更新，所谓同步更新如下图所示

![](http://studentdeng.github.io/images/ml/11.png)

注意: 

1. 当learning rate太大的时候，可能会出现overshoot the minimum现象，类似下图所示情况发生
2. 当函数接近局部最小值的时候，所求偏导数值将会逐渐递减，梯度下降法将自动的采取“小步子”， 所以没有必要随着时间的推移减小learning rate.

![](http://img.my.csdn.net/uploads/201209/06/1346902300_4179.png)

## 3.梯度下降-线性回归
还是假设只有2个参数线性回归模型

1. 左边是线性回归的模型，hθ(x)表示假设函数，J(θ0,θ1)则表示的是代价函数
2. 右边是梯度下降的算法定义

![](http://52opencourse.com/?qa=blob&qa_blobid=9453514901512885273)
![](http://52opencourse.com/?qa=blob&qa_blobid=1147990197831711062)
 
J(θ0,θ1)对于θ0和θ1分别求导，可得

![](http://studentdeng.github.io/images/ml/15.png)

展开过程如下所示
![](http://img.blog.csdn.net/20160731222318375)

特别注意: θ0和θ1的值需要同步更新

## 4.梯度下降-多个参数线性回归
很多时候，我们的假设函数hθ(x)不止2个参数，可能会有n+1个参数θ0,θ1,θ2 ... θn，此时对应特征向量x有n维

因此我们可以得到n个参数的线性回归模型

![](http://studentdeng.github.io/images/ml/4.png)

对于这个n个参数的线性回归函数，我们可以得到对应的梯度下降算法

![](http://studentdeng.github.io/images/ml/19.png)

展开求偏导数后，可得

![](http://studentdeng.github.io/images/ml/17.png)

总结： 这里的梯度下降算法也称为”Batch” 梯度下降: 因为梯度下降的每一步都使用了所有的训练样本。

