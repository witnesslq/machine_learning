## 1. 逻辑回归
监督学习中另一个问题为分类问题，常见的分类问题例子有

1. 邮件是否是垃圾邮件，0表示垃圾邮件，1表示正常邮件
2. 在线交易是否会欺骗用户，0表示会欺骗，1表示不会
3. 患肿瘤患者是良性还是恶性，0表示恶性，1表示良性

这些问题，可以归之于`二分类问题`，y表示因变量，取值0和1，可以定义如下

![](http://52opencourse.com/?qa=blob&qa_blobid=8298951987645335658)

其中0表示负例，1表示正例
同理，对于多分类问题来说，因变量y的值可以取{0,1,2,3 ... n}

我们先从`二分类问题`入手，理解什么是逻辑回归模型

逻辑回归（Logistic Regression）是一种用于解决二分类问题（0 or 1）的机器学习模型
逻辑回归（Logistic Regression）与线性回归（Linear Regression）都是一种广义线性模型（generalized linear model）逻辑回归假设因变量y 服从`伯努利分布`，而线性回归假设因变量y服从`高斯分布`

## 2. 逻辑回归模型
线性回归模型中，假设函数h(x)预测的值是连续的，预测值在`[负无穷, 正无穷]`

对于二分类问题来说最终的预测值是{0,1}离散集合，我们需要找出一个预测函数模型，使其值的输出在 [0, 1] 之间。然后我们选择一个阀值d，比如 0.5 ，如果预测值算出来大于 0.5 就认为分类结果为1，反之则认为分类结果为0.

因此我们引入了函数g, 同时假设逻辑回归模型的假设函数h(x)如下

![](https://github.com/endymecy/spark-ml-source-analysis/raw/master/%E5%88%86%E7%B1%BB%E5%92%8C%E5%9B%9E%E5%BD%92/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/imgs/1.4.png)

这个函数称为Sigmoid函数，也称为逻辑函数（Logistic function） 函数曲线如下

![](http://img.blog.csdn.net/20160409203837285)

线性回归模型假设函数h(x) = (theta^T * x), (theta^T * x)值域在`[负无穷, 正无穷]`，函数h(x) = g(theta^T * x)预测值在[0, 1]之间

我们可以设置一个阈值d，当h(x) < d的时候预测分类结果为0，当h(x) >= d的时候预测分类结果为1

假设阀值d = 0.5

1. 当h(x) >= 0.5的时候，则预测分类结果为1
2. 当h(x) < 0.5的时候，则预测分类结果为0

因此函数h(x)的输出可以解释为`分类结果为1时的概率`

![](https://github.com/endymecy/spark-ml-source-analysis/raw/master/%E5%88%86%E7%B1%BB%E5%92%8C%E5%9B%9E%E5%BD%92/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/imgs/1.5.png)

## 3. 决策边界
假设阀值d=0.5, 当h(x) >= 0.5的时候，预测分类结果为1

从逻辑函数的曲线图可知，对于函数h(x) = g(theta^T * x) >= 0.5 等价于theta^T * x >= 0

我们称 `theta^T * x = 0` 是模型的`决策边界`，大于0的时候预测分类结果为1，小于0的时候预测分类结果为0.

举个例子
假设h(x) = g(theta0 + theta1*x1 + theta2*x2)，theta0, theta1, theta2 分别取-3, 1, 1
由上可知，决策边界 `-3 + x1 + x2 = 0` 是一个线性的方程，如下图所示

![](http://52opencourse.com/?qa=blob&qa_blobid=17042785878272231122)

当h(x)更加复杂的时候，决策边界可能是一个非线性的方程

![](http://52opencourse.com/?qa=blob&qa_blobid=6779199348343391320)

![](http://52opencourse.com/?qa=blob&qa_blobid=14854555397734057469)

