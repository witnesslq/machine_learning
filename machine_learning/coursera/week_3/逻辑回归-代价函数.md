## 1. 引言
回到线性回归模型中，训练集和代价函数如下图

![](http://images.cnitblog.com/blog/575572/201311/09081133-fe9c33298fe44030b121be27a7d2d493.png)

![](https://camo.githubusercontent.com/69d7473a15e3ebc5f447bdf7d3091cc2eb0a4f8e/687474703a2f2f696d672e626c6f672e6373646e2e6e65742f3230313630343138313931333030333836)

如果我们还用J(θ)函数做为逻辑回归模型的代价函数，用H(x) = g(θ^T * x)，曲线如下图所示

发现J(θ)的曲线图是"非凸函数"，存在多个局部最小值，不利于我们求解全局最小值

![](http://52opencourse.com/?qa=blob&qa_blobid=607435295049781725)

因此，上述的`代价函数`对于逻辑回归是不可行的，我们需要其他形式的`代价函数`来保证逻辑回归的`代价函数`是凸函数。

## 2. 代价函数
这里我们先对线性回归模型中的代价函数J(θ)进行简单的改写

![](http://images.cnitblog.com/blog/575572/201311/09081405-9b492cc9537d4e6bb4a979aaf640e862.png)

用Cost(h(x), y) = 1/2(h(x) - y)^2 代替

![](http://images.cnitblog.com/blog/575572/201311/09081437-30ae997c4ec8401a9daf276cede74bc7.png)

在这里我们选择`对数似然损失函数`做为逻辑回归模型的`代价函数`，Cost函数可以表示如下

![](http://images.cnitblog.com/blog/575572/201311/09081713-0a2cb0a314a243419a5b31b3f3729134.png)

分析下这个代价函数

(1). 当y=1的时候，Cost(h(x), y) = -log(h(x))。h(x)的值域0~1，-log(h(x))的曲线图，如下

![](http://images.cnitblog.com/blog/575572/201401/261435121104911.png)

从图中可以看出

1. h(x)的值趋近于1的时候，代价函数的值越小趋近于0，也就是说预测的值h(x)和训练集结果y=1越接近，预测错误的代价越来越接近于0，分类结果为1的概率为1
2. 当h(x)的值趋近于0的时候，代价函数的值无穷大，也就说预测的值h(x)和训练集结果y=1越相反，预测错误的代价越来越趋于无穷大，分类结果为1的概率为0

(2). 当y=0的时候， Cost(h(x), y) = -log(1-h(x))。h(x)的值域0~1，-log(1-h(x))的曲线图，如下

![](http://images.cnitblog.com/blog/575572/201401/261435219076718.png)

从图中可以看出

1. h(x)的值趋近于1的时候，代价函数的值趋于无穷大，也就是说预测的值h(x)和训练集结果y=0越相反，预测错误的代价越来越趋于无穷大，分类结果为0的概率为1-h(x)等于0
2. 当h(x)的值趋近于0的时候，代价函数的值越小趋近于0，也就说预测的值h(x)和训练集结果y=0越接近，预测错误的代价越来越接近于0，分类结果为0的概率为1-h(x)等于1

为了统一表示，可以把Cost(h(x), y)表达成统一的式子，根据前面J(θ)的定义，J(θ)等于

![](http://img.it610.com/image/info5/e77430b14cf544ed88b4294af926b6c5.png)

特别说明:

1. 当y=1的时候，第二项(1-y)*log(1-h(x))等于0
2. 当y=0的时候，y*log(h(x))等于0

从上面2点可以看出，J(θ)表达式符合前面定义

根据线性回归求代价函数的方法，可以用`梯度下降算法`求解参数θ

![](http://img.it610.com/image/info5/f2c93802bc3344adbdb487409603f0da.jpg)

![](https://camo.githubusercontent.com/a67768300ee2d427a94eecc43ccb6c0afe5f194b/687474703a2f2f73747564656e7464656e672e6769746875622e696f2f696d616765732f6d6c2f31372e706e67)

从上图可以看出，θj更新和线性回归中梯度下降算法的θj更新一致，差别的是假设函数h(x)不同
