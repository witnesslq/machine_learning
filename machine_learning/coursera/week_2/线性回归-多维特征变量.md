## 1. 假设函数
之前的几篇文章里面，我们都只是介绍了单维特征变量的线性回归模型，比如预测房价的时候，我们只用了房子的面积这个维度。

接下来我们会去研究多个维度的线性回归模型

还是从预测房价这个例子入手，假设我们现在不只是单纯的考虑房子的面积，还考虑了卧室的数量、楼层、房子年限等三个维数

![](http://img.blog.csdn.net/20150809104424214)

由于特征向量x的维度是多维，因此我们的表示发生了一些变化，如下图

![](http://img.blog.csdn.net/20160418190404046)

因此，多个维度特征变量的线性回归的假设函数可定义为

![](http://img.blog.csdn.net/20160418190509656)

还是假设X0 = 1

![](http://img.blog.csdn.net/20160418190731311)

此时，函数h有n+1个参数θ0 ~ θn，同时特征向量x有n维，x1 ~ xn，特殊的是x0永远等于1

不难发现函数h是特征向量x(x0,x1 ... xn) 和 参数θ的转置矩阵的乘积，证明如下

![](http://img.blog.csdn.net/20150809104445910)

因此，函数h可以简化为如下式子

![](http://images.cnitblog.com/blog/663864/201410/272157205655514.png)

## 2. 代价函数
同理，扩展到多维特征变量之后，代价函数J，如下所示

![](http://img.blog.csdn.net/20160418191300386)

我们的目的也是通过多轮的迭代，找到最佳的参数θ0 ~ θn，使得函数J(θ0,θ1,...θn)的值最小

![](http://img.blog.csdn.net/20160418191511204)

![](http://img.blog.csdn.net/20160418191555080)

![](http://img.blog.csdn.net/20160418191842909)

